{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "\n",
    "So we can use big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, files, batch_size):\n",
    "        self.files = files\n",
    "        self.batch_size = batch_size\n",
    "        file_lengths = [len(np.load(f)) for f in files]\n",
    "        self.total_length = sum(file_lengths)\n",
    "        self.cumulative_lengths = np.cumsum([0] + file_lengths)\n",
    "        self.file_num = 0\n",
    "        self.file_cache = pd.DataFrame(np.load(self.files[self.file_num])).sample(frac=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.total_length) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Start and end lengths of batch\n",
    "        global_start = idx * self.batch_size\n",
    "        global_end = global_start + self.batch_size\n",
    "\n",
    "        # Get global bounds of current file\n",
    "        fileLowerBound, fileUpperBound = self.cumulative_lengths[self.file_num], self.cumulative_lengths[self.file_num + 1]\n",
    "\n",
    "        # check that the global_start and global_end are within the bounds of the current  file\n",
    "        # If not, then enter this. Here, we fetch the next file.\n",
    "        if global_end > fileUpperBound:\n",
    "            self.file_num  += 1\n",
    "            self.file_cache = pd.DataFrame()  # Clear the cache by setting it to an empty DataFrame\n",
    "            self.file_cache = pd.DataFrame(np.load(self.files[self.file_num]), dtype=np.int16).sample(frac=1)\n",
    "\n",
    "        # Enter this every epoch, or when batch number resets to 0        \n",
    "        if global_start < fileLowerBound and global_start == 0:\n",
    "            self.file_num = 0\n",
    "            self.file_cache = pd.DataFrame()  # Clear the cache by setting it to an empty DataFrame\n",
    "            self.file_cache = pd.DataFrame(np.load(self.files[self.file_num])).sample(frac=1)\n",
    "\n",
    "        fileLowerBound, fileUpperBound = self.cumulative_lengths[self.file_num], self.cumulative_lengths[self.file_num + 1]\n",
    "\n",
    "        \n",
    "        local_start = global_start - fileLowerBound\n",
    "        local_end = local_start + self.batch_size\n",
    "        data = self.file_cache.iloc[local_start:local_end]\n",
    "\n",
    "        x = data.iloc[:, :-1].to_numpy().astype(np.int8)\n",
    "        y = data.iloc[:, -1].to_numpy().astype(np.int16)\n",
    "\n",
    "        return (x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Model\n",
    "\n",
    "Need to rerun the processedDataset, such that the evaluations are correct.\n",
    "\n",
    "PARAMETERS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Reshape((8, 8, 13), input_shape=(832,)),\n",
    "    # tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
    "    # tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    # tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'),\n",
    "    # tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    # tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1024, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1024, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1024, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1, activation='linear', kernel_initializer='he_normal')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, clipnorm=1.0)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, clipnorm = 1.0)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='MeanSquaredError', metrics=['MeanAbsoluteError'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8192\n",
    "files = ['./../data/npyV1/mixedDataChunk' + str(num) +'.npy' for num in np.arange(13)]\n",
    "\n",
    "# Make the DataSequence object to pass data to model\n",
    "train = DataSequence(files[0:5], batch_size = batch_size)\n",
    "\n",
    "valid = DataSequence(files[5:6], batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "610/610 [==============================] - 25s 38ms/step - loss: 248119.0781 - mean_absolute_error: 263.9427 - val_loss: 220305.8594 - val_mean_absolute_error: 256.9581 - lr: 0.0100\n",
      "Epoch 2/60\n",
      "610/610 [==============================] - 24s 36ms/step - loss: 208637.4531 - mean_absolute_error: 251.5627 - val_loss: 193414.7812 - val_mean_absolute_error: 242.0929 - lr: 0.0100\n",
      "Epoch 3/60\n",
      "610/610 [==============================] - 23s 34ms/step - loss: 193946.5938 - mean_absolute_error: 244.6238 - val_loss: 185745.8125 - val_mean_absolute_error: 236.0078 - lr: 0.0100\n",
      "Epoch 4/60\n",
      "610/610 [==============================] - 24s 37ms/step - loss: 185976.0938 - mean_absolute_error: 239.3477 - val_loss: 193547.3125 - val_mean_absolute_error: 257.6861 - lr: 0.0100\n",
      "Epoch 5/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 178096.1406 - mean_absolute_error: 233.4611 - val_loss: 182773.7500 - val_mean_absolute_error: 245.7703 - lr: 0.0100\n",
      "Epoch 6/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 168907.2500 - mean_absolute_error: 227.0081 - val_loss: 194992.8438 - val_mean_absolute_error: 265.7299 - lr: 0.0100\n",
      "Epoch 7/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 166611.1094 - mean_absolute_error: 226.6661 - val_loss: 187138.2188 - val_mean_absolute_error: 257.5131 - lr: 0.0100\n",
      "Epoch 8/60\n",
      "610/610 [==============================] - 24s 36ms/step - loss: 164182.4219 - mean_absolute_error: 225.9166 - val_loss: 166339.0938 - val_mean_absolute_error: 228.5865 - lr: 0.0100\n",
      "Epoch 9/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 156506.0781 - mean_absolute_error: 220.3568 - val_loss: 165233.4219 - val_mean_absolute_error: 230.7865 - lr: 0.0100\n",
      "Epoch 10/60\n",
      "610/610 [==============================] - 24s 36ms/step - loss: 148867.7344 - mean_absolute_error: 215.2059 - val_loss: 157892.2812 - val_mean_absolute_error: 229.4745 - lr: 0.0100\n",
      "Epoch 11/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 143560.0469 - mean_absolute_error: 212.1159 - val_loss: 148336.0938 - val_mean_absolute_error: 214.9079 - lr: 0.0100\n",
      "Epoch 12/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 140632.1250 - mean_absolute_error: 211.0785 - val_loss: 144766.3750 - val_mean_absolute_error: 208.3872 - lr: 0.0100\n",
      "Epoch 13/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 138940.9375 - mean_absolute_error: 210.7825 - val_loss: 155954.9531 - val_mean_absolute_error: 226.1007 - lr: 0.0100\n",
      "Epoch 14/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 132090.7969 - mean_absolute_error: 205.2549 - val_loss: 141797.3594 - val_mean_absolute_error: 205.7821 - lr: 0.0100\n",
      "Epoch 15/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 129622.0234 - mean_absolute_error: 204.3582 - val_loss: 153609.9375 - val_mean_absolute_error: 224.6944 - lr: 0.0100\n",
      "Epoch 16/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 123042.8281 - mean_absolute_error: 199.4483 - val_loss: 132176.6094 - val_mean_absolute_error: 201.0466 - lr: 0.0100\n",
      "Epoch 17/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 117595.6953 - mean_absolute_error: 195.3791 - val_loss: 132859.0938 - val_mean_absolute_error: 201.0086 - lr: 0.0100\n",
      "Epoch 18/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 117174.7422 - mean_absolute_error: 196.2780 - val_loss: 130419.8906 - val_mean_absolute_error: 196.3181 - lr: 0.0100\n",
      "Epoch 19/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 111382.1484 - mean_absolute_error: 191.5934 - val_loss: 126980.2188 - val_mean_absolute_error: 195.0240 - lr: 0.0100\n",
      "Epoch 20/60\n",
      "610/610 [==============================] - 24s 35ms/step - loss: 105987.1562 - mean_absolute_error: 187.2561 - val_loss: 124401.9062 - val_mean_absolute_error: 192.8518 - lr: 0.0100\n",
      "Epoch 21/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 106842.2109 - mean_absolute_error: 189.5420 - val_loss: 141447.3125 - val_mean_absolute_error: 210.2348 - lr: 0.0100\n",
      "Epoch 22/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 103060.3281 - mean_absolute_error: 187.0270 - val_loss: 120955.2969 - val_mean_absolute_error: 195.7776 - lr: 0.0100\n",
      "Epoch 23/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 98084.1406 - mean_absolute_error: 182.3387 - val_loss: 121696.6953 - val_mean_absolute_error: 193.2952 - lr: 0.0100\n",
      "Epoch 24/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 94109.2188 - mean_absolute_error: 178.6719 - val_loss: 121044.4297 - val_mean_absolute_error: 192.0774 - lr: 0.0100\n",
      "Epoch 25/60\n",
      "609/610 [============================>.] - ETA: 0s - loss: 90060.5391 - mean_absolute_error: 175.0385\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 90068.7031 - mean_absolute_error: 175.0479 - val_loss: 121715.0312 - val_mean_absolute_error: 192.4590 - lr: 0.0100\n",
      "Epoch 26/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 82622.0938 - mean_absolute_error: 168.3271 - val_loss: 110739.4375 - val_mean_absolute_error: 180.7576 - lr: 1.0000e-03\n",
      "Epoch 27/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 78949.2109 - mean_absolute_error: 164.6677 - val_loss: 110379.4766 - val_mean_absolute_error: 179.8568 - lr: 1.0000e-03\n",
      "Epoch 28/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 78124.8984 - mean_absolute_error: 163.9461 - val_loss: 111177.1797 - val_mean_absolute_error: 181.9887 - lr: 1.0000e-03\n",
      "Epoch 29/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 77442.5234 - mean_absolute_error: 163.3868 - val_loss: 109961.8672 - val_mean_absolute_error: 179.2670 - lr: 1.0000e-03\n",
      "Epoch 30/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 76839.9375 - mean_absolute_error: 162.8696 - val_loss: 110012.8594 - val_mean_absolute_error: 179.6615 - lr: 1.0000e-03\n",
      "Epoch 31/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 76145.6719 - mean_absolute_error: 162.2490 - val_loss: 109809.1719 - val_mean_absolute_error: 180.2582 - lr: 1.0000e-03\n",
      "Epoch 32/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 75574.4375 - mean_absolute_error: 161.7838 - val_loss: 109216.5469 - val_mean_absolute_error: 178.3977 - lr: 1.0000e-03\n",
      "Epoch 33/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 75029.1719 - mean_absolute_error: 161.3284 - val_loss: 109876.9922 - val_mean_absolute_error: 179.9834 - lr: 1.0000e-03\n",
      "Epoch 34/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 74401.4609 - mean_absolute_error: 160.7734 - val_loss: 109087.5781 - val_mean_absolute_error: 178.8291 - lr: 1.0000e-03\n",
      "Epoch 35/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 73823.3438 - mean_absolute_error: 160.2702 - val_loss: 109155.8516 - val_mean_absolute_error: 178.9951 - lr: 1.0000e-03\n",
      "Epoch 36/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 73369.1484 - mean_absolute_error: 159.9093 - val_loss: 109515.3516 - val_mean_absolute_error: 179.6983 - lr: 1.0000e-03\n",
      "Epoch 37/60\n",
      "610/610 [==============================] - 23s 34ms/step - loss: 72818.8984 - mean_absolute_error: 159.4312 - val_loss: 108271.8281 - val_mean_absolute_error: 177.3752 - lr: 1.0000e-03\n",
      "Epoch 38/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 72315.5156 - mean_absolute_error: 159.0012 - val_loss: 108260.8828 - val_mean_absolute_error: 177.1688 - lr: 1.0000e-03\n",
      "Epoch 39/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 71817.7422 - mean_absolute_error: 158.4969 - val_loss: 108563.6016 - val_mean_absolute_error: 177.5611 - lr: 1.0000e-03\n",
      "Epoch 40/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 71438.3672 - mean_absolute_error: 158.2684 - val_loss: 108332.1250 - val_mean_absolute_error: 177.8900 - lr: 1.0000e-03\n",
      "Epoch 41/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 70912.0547 - mean_absolute_error: 157.8095 - val_loss: 108219.1797 - val_mean_absolute_error: 177.5492 - lr: 1.0000e-03\n",
      "Epoch 42/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 70396.7344 - mean_absolute_error: 157.3165 - val_loss: 108634.8203 - val_mean_absolute_error: 178.9984 - lr: 1.0000e-03\n",
      "Epoch 43/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 70055.0859 - mean_absolute_error: 157.0162 - val_loss: 107707.5391 - val_mean_absolute_error: 176.7761 - lr: 1.0000e-03\n",
      "Epoch 44/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 69586.2188 - mean_absolute_error: 156.6117 - val_loss: 108256.7656 - val_mean_absolute_error: 178.9173 - lr: 1.0000e-03\n",
      "Epoch 45/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 69050.5156 - mean_absolute_error: 156.1770 - val_loss: 107466.6562 - val_mean_absolute_error: 176.2212 - lr: 1.0000e-03\n",
      "Epoch 46/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 68753.1250 - mean_absolute_error: 155.9609 - val_loss: 107877.3984 - val_mean_absolute_error: 178.1107 - lr: 1.0000e-03\n",
      "Epoch 47/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 68255.9453 - mean_absolute_error: 155.5303 - val_loss: 107470.8203 - val_mean_absolute_error: 176.5841 - lr: 1.0000e-03\n",
      "Epoch 48/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 67878.6953 - mean_absolute_error: 155.1701 - val_loss: 107091.0469 - val_mean_absolute_error: 174.9316 - lr: 1.0000e-03\n",
      "Epoch 49/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 67446.9141 - mean_absolute_error: 154.7679 - val_loss: 107032.6875 - val_mean_absolute_error: 175.6368 - lr: 1.0000e-03\n",
      "Epoch 50/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 67020.9766 - mean_absolute_error: 154.4079 - val_loss: 106712.9062 - val_mean_absolute_error: 174.9371 - lr: 1.0000e-03\n",
      "Epoch 51/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 66713.0938 - mean_absolute_error: 154.1095 - val_loss: 107554.7188 - val_mean_absolute_error: 178.1391 - lr: 1.0000e-03\n",
      "Epoch 52/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 66387.4219 - mean_absolute_error: 153.8240 - val_loss: 106520.7656 - val_mean_absolute_error: 175.4816 - lr: 1.0000e-03\n",
      "Epoch 53/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 65956.9062 - mean_absolute_error: 153.4162 - val_loss: 106923.4688 - val_mean_absolute_error: 175.8375 - lr: 1.0000e-03\n",
      "Epoch 54/60\n",
      "610/610 [==============================] - 23s 34ms/step - loss: 65545.6406 - mean_absolute_error: 153.0702 - val_loss: 106721.0391 - val_mean_absolute_error: 175.8194 - lr: 1.0000e-03\n",
      "Epoch 55/60\n",
      "608/610 [============================>.] - ETA: 0s - loss: 65234.0078 - mean_absolute_error: 152.8492\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 65226.7578 - mean_absolute_error: 152.8466 - val_loss: 106694.2109 - val_mean_absolute_error: 174.6826 - lr: 1.0000e-03\n",
      "Epoch 56/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 64279.8477 - mean_absolute_error: 151.7370 - val_loss: 106322.6719 - val_mean_absolute_error: 175.1702 - lr: 1.0000e-04\n",
      "Epoch 57/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 63989.3672 - mean_absolute_error: 151.5206 - val_loss: 106333.8672 - val_mean_absolute_error: 175.2833 - lr: 1.0000e-04\n",
      "Epoch 58/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 63867.3672 - mean_absolute_error: 151.4135 - val_loss: 106211.8047 - val_mean_absolute_error: 174.9863 - lr: 1.0000e-04\n",
      "Epoch 59/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 63803.5508 - mean_absolute_error: 151.3538 - val_loss: 106383.5938 - val_mean_absolute_error: 175.5888 - lr: 1.0000e-04\n",
      "Epoch 60/60\n",
      "610/610 [==============================] - 23s 35ms/step - loss: 63704.7969 - mean_absolute_error: 151.2493 - val_loss: 106200.7891 - val_mean_absolute_error: 174.7138 - lr: 1.0000e-04\n",
      "INFO:tensorflow:Assets written to: ../saved_models/lastlastlastlastmodel\\assets\n"
     ]
    }
   ],
   "source": [
    "# validation checking\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Reduce learning rate\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.00001, verbose=1)\n",
    "\n",
    "model.fit(train, epochs=60, validation_data = valid, callbacks=[early_stopping, reduce_lr], shuffle = False)\n",
    "\n",
    "model.save(\"../saved_models/lastlastlastlastmodel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
