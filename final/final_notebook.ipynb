{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sequenceTest.py\n",
    "\n",
    "Contains code from relevant file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into banches so we can actually process this thing.\n",
    "class DataSequence(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, file, batch_size):\n",
    "        self.file = file\n",
    "        self.batch_size = batch_size\n",
    "      \n",
    "    def __len__(self):\n",
    "        #total_length = sum(1 for row in open(self.file))\n",
    "        total_length = 100_000\n",
    "        return int(np.ceil(total_length) / self.batch_size)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        df = pd.read_csv('../data/createdData/preprocessedDataset.csv', nrows = self.batch_size, skiprows=idx*self.batch_size, dtype=np.int8)\n",
    "\n",
    "        X = df.iloc[:, :-1]  # Select all columns except the last one (the features)\n",
    "        y = df.iloc[:, -1]   # Select the last column (the target)\n",
    "\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "        \n",
    "        return (X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training.py\n",
    "\n",
    "Contains code from relevant file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    #tf.keras.layers.Flatten(input_shape=(13, 8, 8)),\n",
    "    tf.keras.layers.Dense(832, activation='linear'),\n",
    "    tf.keras.layers.Dense(832, activation='relu'),\n",
    "    tf.keras.layers.Dense(832, activation='linear'),\n",
    "    tf.keras.layers.Dense(832, activation='relu'),\n",
    "    tf.keras.layers.Dense(832, activation='linear'),\n",
    "    tf.keras.layers.Dense(832, activation='relu'),\n",
    "    #tf.keras.layers.Dense(1, activation=scaled_sigmoid)\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='MeanAbsoluteError')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to used 'vectorized.csv'\n",
    "\n",
    "It is not returning in exactly the way we'd like, so I'm trying to deal with this here. Idk if this will work, that's why I need to try and plug it in to the neural network\n",
    "\n",
    "    UPDATE 12:03 PM 4/28:\n",
    "    \n",
    "    Doesn't work, but figured out why and important information about the data. FEN string arrays are stored such that every single bit has its own column. That is, there are 832 columns we pass into the neural network. Makes sense in hindsight, but it was not something we had nailed down up to this point. This should make understanding the \"failed to convert to tensor from numpyarray\" or whatever a lot easier to understand"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataSequence\n",
    "# train_sequence = DataSequence('../data/createdData/preprocessedDataset.csv', 10_000)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(train_sequence, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "4000/4000 [==============================] - 35s 8ms/step - loss: 7.5533\n",
      "Epoch 2/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 6.1777\n",
      "Epoch 3/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 5.7457\n",
      "Epoch 4/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 5.4939\n",
      "Epoch 5/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 5.3181\n",
      "Epoch 6/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 5.1948\n",
      "Epoch 7/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 5.0717\n",
      "Epoch 8/25\n",
      "4000/4000 [==============================] - 31s 8ms/step - loss: 4.9716\n",
      "Epoch 9/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 4.9054\n",
      "Epoch 10/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 4.8421\n",
      "Epoch 11/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 4.7871\n",
      "Epoch 12/25\n",
      "4000/4000 [==============================] - 34s 8ms/step - loss: 4.7300\n",
      "Epoch 13/25\n",
      "4000/4000 [==============================] - 33s 8ms/step - loss: 4.6602\n",
      "Epoch 14/25\n",
      "4000/4000 [==============================] - 33s 8ms/step - loss: 4.6149\n",
      "Epoch 15/25\n",
      "4000/4000 [==============================] - 33s 8ms/step - loss: 4.5765\n",
      "Epoch 16/25\n",
      "4000/4000 [==============================] - 33s 8ms/step - loss: 4.5313\n",
      "Epoch 17/25\n",
      "4000/4000 [==============================] - 33s 8ms/step - loss: 4.4983\n",
      "Epoch 18/25\n",
      "4000/4000 [==============================] - 33s 8ms/step - loss: 4.4545\n",
      "Epoch 19/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 4.4144\n",
      "Epoch 20/25\n",
      "4000/4000 [==============================] - 33s 8ms/step - loss: 4.3905\n",
      "Epoch 21/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 4.3500\n",
      "Epoch 22/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 4.3358\n",
      "Epoch 23/25\n",
      "4000/4000 [==============================] - 33s 8ms/step - loss: 4.2915\n",
      "Epoch 24/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 4.2600\n",
      "Epoch 25/25\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 4.2380\n",
      "INFO:tensorflow:Assets written to: ../saved_models/saved_model_4mRows_25Epochs_1kBatchSize\\assets\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/createdData/preprocessedDataset.csv', nrows = 4_000_000, dtype=np.int8)\n",
    "\n",
    "X = df.iloc[:, :-1]  # Select all columns except the last one (the features)\n",
    "y = df.iloc[:, -1]   # Select the last column (the target)\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "model.fit(X, y, epochs=25, batch_size=1_000)\n",
    "\n",
    "model.save('../saved_models/saved_model_4mRows_25Epochs_1kBatchSize')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
